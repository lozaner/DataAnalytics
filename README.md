# Data Analytics

Descripci√≥n General del Portafolio
Bienvenido a mi portafolio de proyectos de an√°lisis de datos con Python. Aqu√≠ encontrar√°s una colecci√≥n de trabajos que reflejan mi trayectoria en el campo, desde mis primeros pasos hasta los proyectos m√°s avanzados que he desarrollado recientemente. Cada proyecto incluye un an√°lisis detallado del problema, el proceso completo de manipulaci√≥n y visualizaci√≥n de datos, y las soluciones implementadas.

Los proyectos est√°n organizados de forma cronol√≥gica inversa, mostrando primero los m√°s recientes para resaltar mi evoluci√≥n y mis habilidades actuales. Mi objetivo con este portafolio es compartir no solo los resultados, sino tambi√©n el aprendizaje y las herramientas que he integrado a lo largo del tiempo, incluyendo bibliotecas como pandas, NumPy, matplotlib, seaborn, y t√©cnicas avanzadas como el modelado predictivo y el aprendizaje autom√°tico.

Este portafolio est√° dise√±ado para demostrar mi capacidad de transformar datos en informaci√≥n valiosa que impulsa decisiones estrat√©gicas. Espero que disfrutes explorando mi trabajo tanto como yo disfrut√© cre√°ndolo.

---
# Proyecto 16 SQL
## Descripci√≥n del proyecto üìäüé•
El proyecto se centra en analizar una base de datos proporcionada por un servicio de biblioteca en l√≠nea, el cual compite en el mercado de libros digitales y f√≠sicos. Esta base de datos contiene informaci√≥n sobre libros, autores, editoriales, calificaciones y rese√±as de usuarios. La finalidad es extraer insights relevantes que permitan dise√±ar una propuesta de valor para un nuevo producto dirigido a los amantes de la lectura, considerando aspectos como tendencias de publicaci√≥n, popularidad de t√≠tulos, desempe√±o de editoriales y la percepci√≥n de los usuarios.

## Preprocesamiento de Datos ‚öôÔ∏èüßπ
Para preparar el an√°lisis se siguieron estos pasos:
- Carga de Datos: Se importaron los datos mediante librer√≠as como Pandas y SQLAlchemy, conect√°ndose a la base de datos SQL.
- Exploraci√≥n y Limpieza:
    - Se revis√≥ la estructura de cada tabla (libros, autores, editoriales, ratings y rese√±as) usando una funci√≥n de resumen que muestra las primeras filas, informaci√≥n general, verificaci√≥n de datos faltantes y duplicados.
    - Se constat√≥ que no exist√≠an valores ausentes ni duplicados en ninguno de los DataFrames, lo que facilit√≥ el an√°lisis posterior.
- Integraci√≥n: Los datos se cargaron y validaron para asegurar su integridad y consistencia antes de proceder con las consultas anal√≠ticas.

## Hip√≥tesis general üìå
La hip√≥tesis central del estudio es que mediante el an√°lisis de la informaci√≥n contenida en la base de datos se podr√°n identificar patrones y tendencias en el comportamiento de los lectores (por ejemplo, en cuanto a la cantidad de libros publicados en la era moderna, popularidad y valoraciones) que sirvan para dise√±ar una propuesta de valor competitiva para un nuevo producto en el mercado de libros. Se parte de la premisa de que el entendimiento profundo de estos datos permitir√° descubrir insights accionables sobre las preferencias y comportamientos de los usuarios, facilitando el desarrollo de un producto que responda eficazmente a sus necesidades.


[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S14-PF-pruebaAB-SQL-Hipotesis/SQL/sql_notas.ipynb)

---
# Proyecto 15 Prueba A/B
## Descripci√≥n del proyecto üìäüé•
El proyecto consiste en la realizaci√≥n de una prueba A/B para una tienda en l√≠nea internacional, en la que se evalu√≥ la implementaci√≥n de un nuevo embudo de pago basado en un sistema de recomendaciones mejorado. Se compararon dos grupos de usuarios:

Grupo A (Control): Usuarios que experimentaron el embudo de pago tradicional.

Grupo B (Experimental): Usuarios que utilizaron el nuevo embudo de pago.

El objetivo central es analizar si el nuevo sistema incrementa las conversiones a lo largo del embudo, que incluye la visita a la p√°gina del producto, la adici√≥n de art√≠culos al carrito y la realizaci√≥n de compras. Se esperaba un aumento m√≠nimo del 10% en cada una de estas etapas en el grupo experimental.

## Preprocesamiento de Datos ‚öôÔ∏èüßπ
El preprocesamiento de datos realizado en el proyecto abarc√≥ las siguientes etapas:

1. Carga de Datos: Se importaron diversos archivos CSV que contienen informaci√≥n sobre eventos de marketing, registros de nuevos usuarios, eventos de usuario y participantes de la prueba.

L2. impieza y Transformaci√≥n:
    - Revisi√≥n y eliminaci√≥n de duplicados en los datasets.
    - Manejo de valores nulos, por ejemplo, en la columna ‚Äúdetails‚Äù del dataset de eventos, donde se reemplazaron los NaN por 0.
    - Conversi√≥n de campos de fecha a formato datetime para facilitar el an√°lisis temporal.

3. Integraci√≥n de Datos: Se realizaron merge entre los datasets (por ejemplo, unir eventos con participantes) utilizando la columna ‚Äúuser_id‚Äù, de modo que se analizara √∫nicamente la informaci√≥n relacionada con la prueba A/B "recommender_system_test".

4. Exploraci√≥n Inicial: Se evalu√≥ la distribuci√≥n de eventos por usuario y se verific√≥ la correcta asignaci√≥n de usuarios a cada grupo (A y B).

## Hip√≥tesis general üìå
La hip√≥tesis general del experimento es que el nuevo sistema de recomendaciones implementado en el embudo de pago influir√° positivamente en las conversiones de los usuarios. Concretamente, se plantea que:

    -  Dentro de los 14 d√≠as posteriores a la inscripci√≥n, los usuarios expuestos al nuevo embudo (Grupo B) presentar√°n un incremento en la conversi√≥n en cada etapa (visitas a la p√°gina del producto, adici√≥n al carrito y compras) de al menos el 10% en comparaci√≥n con el grupo de control (Grupo A).

Esta hip√≥tesis es la base para evaluar si la implementaci√≥n del nuevo sistema mejora la eficiencia del embudo de ventas y, por ende, la experiencia y comportamiento de los usuarios.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S14-PF-pruebaAB-SQL-Hipotesis/PruebaAB/PruebaAB.ipynb)

---
# Proyecto 14 Hipotesis con sugerencias CallMeMaybe
## Descripci√≥n del proyecto üìäüé•
El proyecto "Detecci√≥n de √Åreas de Mejora en el Servicio de Llamadas" tiene como objetivo optimizar la experiencia del cliente en la empresa CallMeMaybe. En este contexto, se busca:

    - Mejorar la eficiencia de los operadores: Identificar √°reas en las que se pueden implementar mejoras para reducir las llamadas perdidas y optimizar los tiempos de espera.
    - An√°lisis de m√©tricas clave: Se analizan indicadores como la duraci√≥n de las llamadas, la distribuci√≥n entre llamadas internas (entre operadores) y llamadas salientes (con clientes), y el porcentaje de llamadas perdidas.
    - Evaluaci√≥n del desempe√±o: Se estudia la relaci√≥n entre el tiempo de espera y el porcentaje de llamadas perdidas, y se segmenta a los operadores en grupos de eficiencia para determinar cu√°les requieren atenci√≥n o capacitaci√≥n.
    - Dashboard interactivo: El proyecto incluye el desarrollo de un dashboard que, mediante filtros de fecha, hora y regi√≥n, permite visualizar gr√°ficos (histogramas, gr√°ficos de pastel, barras, etc.) y responder preguntas clave sobre la popularidad de llamadas, desempe√±o de operadores y √°reas de riesgo.

El objetivo final es implementar mejoras basadas en datos, como la revisi√≥n del sistema de enrutamiento y la capacitaci√≥n espec√≠fica para operadores con bajo rendimiento, adem√°s de planificar pruebas A/B para validar las intervenciones.
## Preprocesamiento de Datos ‚öôÔ∏èüßπ
Para preparar la informaci√≥n que alimenta el dashboard y el an√°lisis, se han seguido estos pasos:

1. Recolecci√≥n de Datos:
    - Los datos provienen de una tabla de agregaci√≥n ubicada en la base de datos "CallMeMaybe", la cual registra informaci√≥n sobre las llamadas, incluyendo la duraci√≥n, la regi√≥n, el tipo de llamada (entrante o saliente), y m√©tricas de llamadas perdidas y tiempos de espera.

2. Limpieza y Verificaci√≥n:
    - Se han revisado los registros para detectar y eliminar duplicados, asegurando que no existan valores nulos o inconsistentes.
    - Se ha verificado que las columnas relacionadas con el tiempo (duraci√≥n, tiempo de espera) y los conteos (n√∫mero de llamadas, porcentaje de llamadas perdidas) est√©n correctamente formateadas.

3. Transformaci√≥n de Datos:
    - Se han convertido las marcas de tiempo a formatos legibles para facilitar el an√°lisis cronol√≥gico y la generaci√≥n de histogramas.
    - Se han agregado y calculado variables derivadas, como el porcentaje de llamadas perdidas por operador y la relaci√≥n entre el tiempo de espera y dicho porcentaje.

4. Agregaci√≥n y Filtrado:
    - Los datos se agruparon por regi√≥n, fecha y tipo de llamada para obtener valores absolutos y porcentajes que alimenten los gr√°ficos del dashboard.

Se prepararon subconjuntos de datos para analizar la eficiencia de los operadores y detectar aquellos con un alto porcentaje de llamadas perdidas o tiempos de espera excesivos.

## Hip√≥tesis general üìå
Con base en el an√°lisis de las m√©tricas y la visualizaci√≥n de los datos, se plantean las siguientes hip√≥tesis:

1. Relaci√≥n entre Tiempo de Espera y Llamadas Perdidas:
    - "A mayor tiempo de espera, mayor ser√° el porcentaje de llamadas perdidas. Reducir los tiempos de espera deber√≠a disminuir la cantidad de llamadas perdidas, mejorando as√≠ la experiencia del cliente."

2. Impacto de las Llamadas Internas en la Eficiencia:
    - "Un elevado porcentaje de llamadas internas (entre operadores) podr√≠a indicar una dependencia excesiva de la comunicaci√≥n interna, lo que puede generar retrasos en la atenci√≥n a clientes. Una redistribuci√≥n o mejora en la comunicaci√≥n interna podr√≠a optimizar la eficiencia."

3. Diferencias en el Desempe√±o de los Operadores:
    - "Los operadores con un porcentaje alto de llamadas perdidas y tiempos de espera prolongados son menos eficientes. Implementar capacitaci√≥n adicional y revisar el sistema de enrutamiento de llamadas puede mejorar significativamente su desempe√±o."

4. Efecto de las Intervenciones en la Experiencia del Cliente:
    - "La aplicaci√≥n de medidas correctivas (como incentivos, revisiones del sistema y seguimiento en tiempo real) conducir√° a una reducci√≥n en las llamadas perdidas y mejorar√° la satisfacci√≥n del cliente, reflejado en un mejor rendimiento de los operadores."

Estas hip√≥tesis guiar√°n la toma de decisiones para implementar estrategias de mejora en el servicio de llamadas, con el objetivo de optimizar tanto la eficiencia operativa como la experiencia general del cliente.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S14-PF-pruebaAB-SQL-Hipotesis/hipotesis-con-sugerencias/hipotesis-con-sugerencias.ipynb)
[Ver Presentacion](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S14-PF-pruebaAB-SQL-Hipotesis/hipotesis-con-sugerencias/dashboard.pdf)

---
# Proyecto 13 Pronosticos de prediccion con machine learning
## Descripci√≥n del proyecto üìäüé•
Este proyecto se centra en analizar y modelar la cancelaci√≥n (churn) de clientes en un gimnasio. El objetivo es identificar los factores que influyen en la retenci√≥n y en la cancelaci√≥n de los clientes, para as√≠ poder dise√±ar estrategias que mejoren la fidelizaci√≥n. Se trabaja con un conjunto de datos que incluye caracter√≠sticas demogr√°ficas y de comportamiento (por ejemplo, edad, contrato, visitas al gimnasio, gasto en servicios adicionales y frecuencia de clases) de 4000 clientes. Adem√°s de un an√°lisis exploratorio y descriptivo, se entrenan modelos de predicci√≥n (como Regresi√≥n Log√≠stica y Bosque Aleatorio) para determinar cu√°l es el mejor m√©todo para anticipar la cancelaci√≥n de clientes. Tambi√©n se realiza un an√°lisis de segmentaci√≥n mediante clustering para agrupar a los clientes en perfiles con diferentes niveles de riesgo de churn, permitiendo as√≠ generar recomendaciones de acciones de retenci√≥n espec√≠ficas para cada grupo.

## Preprocesamiento de Datos ‚öôÔ∏èüßπ
El proceso de preprocesamiento incluy√≥ los siguientes pasos:

1. Carga y Exploraci√≥n Inicial:
- Se importaron las librer√≠as necesarias (Pandas, Matplotlib, Seaborn, Scikit-Learn, entre otras).
- Se carg√≥ el archivo gym_churn_us.csv y se revis√≥ la estructura de los datos con m√©todos como info(), describe() y isna().sum().

2. Limpieza de Datos:
- Se verificaron y corrigieron inconsistencias, asegurando que no existan valores nulos ni duplicados.
- Se estandarizaron los nombres de las columnas (por ejemplo, se convirtieron a min√∫sculas o se ajustaron para mayor claridad).

3. Transformaci√≥n y Preparaci√≥n:

- Se analizaron las distribuciones de variables num√©ricas mediante histogramas y se calcularon estad√≠sticas descriptivas (promedio, desviaci√≥n est√°ndar, etc.).
- Se exploraron correlaciones entre variables con mapas de calor para identificar relaciones potenciales.
- Se dividieron los datos en conjuntos de entrenamiento y validaci√≥n para la construcci√≥n de modelos predictivos.
-Se realizaron transformaciones adicionales, como la estandarizaci√≥n de variables antes de aplicar algoritmos de clustering.

4. Segmentaci√≥n de Clientes:
- Se aplic√≥ un an√°lisis de clustering (usando K-Means y t√©cnicas jer√°rquicas) para segmentar a los clientes en cinco cl√∫steres, permitiendo identificar grupos con distintos perfiles de riesgo en t√©rminos de churn.

## Hip√≥tesis general üìå
A partir del an√°lisis exploratorio y de los resultados obtenidos en la modelizaci√≥n y segmentaci√≥n, se plantean las siguientes hip√≥tesis:

1. Retenci√≥n y Contrato:
    - "Los clientes con contratos de mayor duraci√≥n tienden a cancelar con menor frecuencia, ya que el compromiso a largo plazo se asocia con una mayor fidelidad."
2. Frecuencia de Asistencia:
    - "Una alta frecuencia de visitas y asistencia a clases est√° correlacionada con una menor probabilidad de cancelaci√≥n, mientras que una baja asistencia puede ser un indicador temprano de riesgo de churn."
3. Gasto en Servicios Adicionales:
    - "El mayor gasto en servicios adicionales (como clases extras, masajes o cafeter√≠a) est√° relacionado con una mayor retenci√≥n, ya que estos clientes invierten m√°s en la experiencia del gimnasio."
4. Segmentaci√≥n de Clientes:
    - "La segmentaci√≥n de clientes mediante clustering revelar√° perfiles diferenciados, donde algunos grupos (con mayor compromiso y gasto) mostrar√°n tasas de cancelaci√≥n significativamente menores en comparaci√≥n con aquellos con contratos cortos y menor frecuencia de visitas."

Estas hip√≥tesis buscan guiar las estrategias de retenci√≥n y fidelizaci√≥n, permitiendo dise√±ar intervenciones espec√≠ficas para los distintos segmentos de clientes y, en consecuencia, reducir el churn en el gimnasio.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S13-pronosticos-predicion-machine-learning/model_fitness_gym.ipynb)

---
# Proyecto 12 Uso de Tableau Dash y Pipeline
## Descripci√≥n del proyecto üìäüé•

El proyecto se centra en el an√°lisis de tendencias de videos en YouTube con el objetivo de identificar patrones de popularidad en distintas regiones y categor√≠as. La agencia de publicidad Sterling & Draper busca optimizar su estrategia de marketing mediante un dashboard interactivo que automatice la consulta de datos de tendencias, permitiendo que los gerentes de planificaci√≥n de videos publicitarios accedan a informaci√≥n clave de manera eficiente.

El dashboard proporciona informaci√≥n sobre:
- Las categor√≠as de videos m√°s populares en tendencia.
- La distribuci√≥n de tendencias en distintas regiones.
- La identificaci√≥n de categor√≠as particularmente populares en Estados Unidos.
- La comparaci√≥n de tendencias entre EE.UU. y otros pa√≠ses.
- Para lograr esto, se ha creado una base de datos llamada "youtube", que almacena la tabla "trending_by_time", actualizada diariamente con informaci√≥n sobre la regi√≥n, fecha de tendencia, categor√≠a del video y cantidad de videos en tendencia.

## Preprocesamiento de Datos ‚öôÔ∏èüßπ
  
Para garantizar la precisi√≥n del an√°lisis, se realizaron los siguientes pasos de limpieza y transformaci√≥n de datos:
1. Revisi√≥n y filtrado de datos:
    - Se verific√≥ la existencia de valores nulos o inconsistentes en los campos de fecha, categor√≠a y regi√≥n.
    - Se descartaron registros duplicados para evitar conteos err√≥neos en los gr√°ficos.
2. Conversi√≥n de tipos de datos:
    - Se transformaron las fechas en un formato adecuado para facilitar el an√°lisis cronol√≥gico.
    - Se aseguraron los valores num√©ricos en la columna "videos_count" para permitir c√°lculos precisos.
3. Creaci√≥n de estructuras agregadas:
    - Se agruparon los datos por fecha y categor√≠a para identificar tendencias temporales.
    - Se agregaron m√©tricas de conteo por regi√≥n y categor√≠a para permitir comparaciones entre pa√≠ses.
4. Visualizaci√≥n y validaci√≥n:
    - Se dise√±aron gr√°ficos en Tableau para representar los datos de forma intuitiva.
    - Se realizaron pruebas de acceso y validaci√≥n en distintos navegadores para garantizar la disponibilidad del dashboard.

## Hip√≥tesis general üìå
"Las categor√≠as Entertainment, Music y Howto & Style son las m√°s frecuentes en tendencias a nivel global."
    - Seg√∫n los an√°lisis, estas categor√≠as presentan el mayor n√∫mero de videos en tendencia.

"Las tendencias de videos var√≠an significativamente entre regiones."
    - Se observa que Rusia tiene la mayor proporci√≥n de videos en tendencia, seguida de EE.UU., Francia e India.

"Las categor√≠as populares en EE.UU. difieren de las de otros pa√≠ses."
    - Mientras que en EE.UU. destacan Autos & Vehicles, Comedy y Education, en otras regiones las categor√≠as pueden ser distintas.

"El an√°lisis automatizado mediante dashboards facilita la toma de decisiones en estrategias de marketing."
    - La implementaci√≥n del dashboard optimiza el acceso a la informaci√≥n y reduce el tiempo necesario para responder a preguntas clave sobre tendencias en YouTube.


[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S12-tableau_pipeline_dash/presentacion_dashboard.pdf)

---
# Proyecto 11 Comparacion de Grupos A/B
## Descripci√≥n del proyecto
El proyecto tiene como objetivo analizar y comparar el comportamiento de usuarios en diferentes experimentos (identificados por exp_id: 246, 247 y 248) a partir de un conjunto de eventos registrados en la aplicaci√≥n. La idea principal es evaluar c√≥mo var√≠an las tasas de conversi√≥n y la progresi√≥n en el embudo (desde la aparici√≥n de la pantalla principal hasta la finalizaci√≥n de un pago) entre los distintos grupos. Para ello, se exploran aspectos como:

Volumen de Eventos y Usuarios: Se examina el n√∫mero total de eventos, la cantidad de usuarios √∫nicos y el promedio de eventos por usuario.

Embudo de Conversi√≥n: Se estudia la secuencia de eventos (por ejemplo: MainScreenAppear ‚Üí OffersScreenAppear ‚Üí CartScreenAppear ‚Üí PaymentScreenSuccessful) para determinar en qu√© etapa se pierde mayor cantidad de usuarios y cu√°l es la proporci√≥n que completa el viaje de compra.

Comparaci√≥n de Grupos Experimentales: Se realizan pruebas de diferencia de proporciones entre los grupos de control (246 y 247) y el grupo con fuentes alteradas (248) para determinar si existen diferencias estad√≠sticamente significativas en la ejecuci√≥n de cada evento del embudo.

El an√°lisis se apoya en t√©cnicas de limpieza y exploraci√≥n de datos, visualizaciones (histogramas, gr√°ficos de barras, l√≠neas y scatter plots) y pruebas estad√≠sticas (pruebas de diferencia de proporciones y c√°lculo del estad√≠stico Z).

## Proceso de preprocesamiento
El preprocesamiento se llev√≥ a cabo siguiendo estos pasos:

1. Carga e Iniciaci√≥n:

- Se importaron las librer√≠as necesarias (Pandas, NumPy, matplotlib, seaborn, SciPy, datetime y sidetable).
- Se carg√≥ el archivo ‚Äúlogs_exp_us.csv‚Äù utilizando el separador adecuado, generando un DataFrame con informaci√≥n de eventos, identificadores de usuario, timestamp y el identificador del experimento (exp_id).

2. Exploraci√≥n y Limpieza Inicial:

- Se aplic√≥ la funci√≥n info_gral() para visualizar las primeras filas, conocer la estructura de los datos, revisar la presencia de valores nulos y detectar duplicados.
- Se renombraron las columnas para facilitar el an√°lisis (por ejemplo, 'event', 'user', 'timestamp' y 'exp_id').
- Se eliminaron duplicados y se realiz√≥ la conversi√≥n de la columna de timestamp a formato datetime, creando adem√°s una columna ‚Äúdate‚Äù que agrupa los eventos por d√≠a.

3. Filtrado y Validaci√≥n de la Muestra:

- Se identific√≥ el per√≠odo en el que los datos son m√°s completos (a partir del 1 de agosto de 2019), descartando un peque√±o porcentaje de eventos y usuarios iniciales para evitar sesgos.
- Se verific√≥ la distribuci√≥n de usuarios por grupo experimental y se calcularon indicadores como el n√∫mero total de eventos, el promedio de eventos por usuario y la proporci√≥n de usuarios que participan en cada evento.

4. Construcci√≥n del Embudo y C√°lculo de M√©tricas:

- Se determin√≥ el orden de los eventos clave del embudo (por ejemplo: MainScreenAppear, OffersScreenAppear, CartScreenAppear y PaymentScreenSuccessful).
- Se calcularon las transiciones entre eventos para conocer la frecuencia con la que los usuarios pasan de una etapa a la siguiente y se estimaron las proporciones de conversi√≥n en cada paso.
- Se analizaron las p√©rdidas en cada etapa (por ejemplo, se identific√≥ que la mayor p√©rdida ocurre desde la pantalla principal, con un abandono aproximado del 38%).

5. Comparaci√≥n de Grupos y Pruebas Estad√≠sticas:

-Se evaluaron las tasas de conversi√≥n en cada grupo experimental (control 246, control 247 y el grupo 248 con fuentes alteradas).
-Se realizaron pruebas de diferencia de proporciones (usando el estad√≠stico Z y el p-valor) tanto entre los grupos de control como comparando estos con el grupo alterado.
- Adem√°s, se desarroll√≥ una funci√≥n para automatizar las comparaciones por cada tipo de evento, verificando que la asignaci√≥n de los usuarios entre grupos sea homog√©nea.

## Hip√≥tesis general
El an√°lisis busca confirmar si los grupos de control (246 y 247) se dividen de forma equitativa y si el grupo con fuentes alteradas (248) presenta diferencias en la conversi√≥n o en la ejecuci√≥n de eventos. Entre los hallazgos destacan:

1. Embudo de Conversi√≥n:
- Se observa que la etapa ‚ÄúMainScreenAppear‚Äù es la que presenta la mayor p√©rdida de usuarios (38.09%).
- Solo el 47.81% de los usuarios que comienzan el embudo (MainScreenAppear) llegan a completar el proceso de pago.

2. Comparaci√≥n de Grupos de Control:
- Las pruebas de diferencia de proporciones entre los grupos 246 y 247 no muestran diferencias estad√≠sticamente significativas, lo que indica una asignaci√≥n homog√©nea de usuarios entre ellos.
- Comparaci√≥n con Grupo Alterado (248):
- Al comparar el grupo 248 contra cada uno de los grupos de control y contra el grupo combinado, no se encontraron diferencias significativas en las proporciones de usuarios que realizan cada uno de los eventos (incluyendo MainScreenAppear, OffersScreenAppear, CartScreenAppear y PaymentScreenSuccessful).

3. Nivel de Significancia:
- Se estableci√≥ un nivel de significancia de 0.05 y se realizaron 15 pruebas en total, lo que respalda la robustez de las conclusiones ante el riesgo de falsos positivos.

4. Conclusi√≥n General:
- El an√°lisis demuestra que las muestras de los grupos de control est√°n bien divididas y que, en comparaci√≥n, el grupo con fuentes alteradas (248) no presenta diferencias significativas en t√©rminos de conversi√≥n ni en el embudo de eventos. Se concluye que las estrategias o fuentes aplicadas en el grupo 248 no afectan de manera significativa el comportamiento de los usuarios en comparaci√≥n con los grupos de control.

5. Recomendaciones:
- Prestar especial atenci√≥n a la alta p√©rdida de usuarios en la etapa de ‚ÄúMainScreenAppear‚Äù para mejorar el embudo de conversi√≥n.
- Considerar la realizaci√≥n de nuevas pruebas A/B con cambios m√°s dr√°sticos o modificaciones en la interfaz para ver si se pueden generar mejoras en la conversi√≥n global.
- Mantener el nivel de significancia en 0.05 o, de ser necesario, evaluarlo con un umbral m√°s estricto (por ejemplo, 0.01) para minimizar los falsos positivos, aunque en este caso los resultados se mantienen sin diferencias significativas.


[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S11-analisis-producto-alimenticio%20prueba%20AB/comparacion-grupos.ipynb)

---
# Proyecto 10 Prueba A/B

## Descripci√≥n del proyecto
El objetivo principal del proyecto es analizar los resultados de un test A/B aplicado sobre un conjunto de datos provenientes de √≥rdenes y visitas. Se busca evaluar la efectividad de dos estrategias diferentes ‚Äìidentificadas como Grupo A y Grupo B‚Äì mediante el estudio de indicadores clave como:

- Tasa de Conversi√≥n: Relaci√≥n entre el n√∫mero de pedidos y las visitas diarias.
- Tama√±o Promedio de Pedido: Valor promedio de las transacciones.
- Ganancia Acumulada: Ingresos totales generados a lo largo del tiempo por cada grupo.

La meta es determinar si existe una diferencia estad√≠sticamente significativa entre ambos grupos y, en caso afirmativo, recomendar la implementaci√≥n de la estrategia que demuestre mejores resultados en t√©rminos de conversi√≥n y rentabilidad.

## Proceso de preprocesamiento
1. Importaci√≥n de Librer√≠as y Carga de Datos:

Se importaron librer√≠as fundamentales como Pandas, NumPy, matplotlib, seaborn, SciPy (stats) y datetime.
Se cargaron tres conjuntos de datos:

- hipotesis: Contiene las propuestas y sus indicadores (Reach, Impact, Confidence, Effort) para priorizar ideas mediante los frameworks ICE y RICE.
- ordenes: Registra las transacciones con informaci√≥n sobre ID, fecha, revenue y asignaci√≥n a grupo (A o B).
- visitantes: Contiene el n√∫mero de visitas diarias por grupo.

2. Exploraci√≥n y Limpieza Inicial:

- Se defini√≥ una funci√≥n de revisi√≥n (info_gral) que permiti√≥ visualizar las primeras filas, tipos de datos, presencia de valores nulos y duplicados en cada dataset.
- Se convirtieron las columnas de fecha a formato datetime para garantizar la correcta manipulaci√≥n temporal de los datos.

Depuraci√≥n e Integraci√≥n:

- Se identificaron y eliminaron usuarios que aparec√≠an en ambos grupos, asegurando que cada usuario fuese asignado a un √∫nico grupo de prueba para evitar sesgos en el an√°lisis.
- Se calcularon indicadores ICE y RICE para priorizar las hip√≥tesis y establecer un orden de importancia en funci√≥n de impacto, alcance, confianza y esfuerzo.

3. Generaci√≥n de Variables Derivadas y M√©tricas:

- Se calcularon m√©tricas acumuladas: n√∫mero de transacciones, visitas acumuladas y revenue acumulado para cada grupo, generando tablas pivot y series temporales.
- Se derivaron variables como la tasa de conversi√≥n diaria (pedidos/visitas) y el tama√±o promedio de pedido, as√≠ como medidas para identificar anomal√≠as (utilizando percentiles 95 y 99).

4. Visualizaci√≥n y An√°lisis Exploratorio:

- Se generaron gr√°ficos para visualizar y comparar:
- La rentabilidad acumulada de los grupos A y B.
- El n√∫mero de transacciones acumuladas y su evoluci√≥n.
- La diferencia relativa en el tama√±o de pedido promedio entre los grupos.
- La tasa de conversi√≥n diaria para identificar tendencias y diferencias significativas.
- Adem√°s, se realiz√≥ un an√°lisis de dispersi√≥n y se calcularon percentiles para detectar comportamientos at√≠picos.

## Hip√≥tesis general
El an√°lisis se centr√≥ en determinar si las diferencias observadas entre los grupos A y B eran estad√≠sticamente significativas. Entre los hallazgos clave se destacan:

1. Conversi√≥n:

- La prueba de Mann-Whitney aplicada a los datos (despu√©s de filtrar usuarios an√≥malos) arroj√≥ un p-valor menor a 0.05 (p ‚âà 0.00879), lo que indica que el Grupo B presenta una tasa de conversi√≥n significativamente mayor que el Grupo A.
- La diferencia relativa en conversi√≥n se estim√≥ en aproximadamente un 18.3%, lo que sugiere que la estrategia implementada en el Grupo B es m√°s efectiva para convertir visitas en transacciones.

2. Tama√±o Promedio de Pedido:

- Aunque el Grupo B mostr√≥ un tama√±o promedio de pedido ligeramente inferior (alrededor de un 4.2% menor) en comparaci√≥n con el Grupo A, la diferencia no fue estad√≠sticamente significativa (p ‚âà 0.71053).

3. Ganancia Total:

- El Grupo B gener√≥ una ganancia acumulada superior a lo largo del per√≠odo analizado, lo que respalda la superioridad de su estrategia en t√©rminos de ingresos totales.
- Decisi√≥n y Recomendaci√≥n:
Con base en los resultados, se recomienda detener el test A/B y considerar al Grupo B como la estrategia l√≠der. La evidencia sugiere que la mejora significativa en la conversi√≥n (y, por ende, en la ganancia acumulada) es el factor determinante, a pesar de que el tama√±o promedio de pedido no muestra diferencias significativas.

Esta decisi√≥n se justifica por:

- Mayor eficacia en conversi√≥n: El Grupo B convierte un mayor porcentaje de visitas en transacciones.
- Rentabilidad acumulada: La ganancia total del Grupo B supera a la del Grupo A.
- Estabilidad de resultados: Los an√°lisis gr√°ficos y estad√≠sticos confirman la consistencia del desempe√±o superior del Grupo B.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S10-hipotesis-prueba%20AB/hipotesis%20AB.ipynb)

---
# Proyecto 9 Analisis de Negocio 

## Descripci√≥n del proyecto
El proyecto se enfoca en el an√°lisis de datos de una empresa dedicada a la venta de entradas para eventos. El objetivo principal es optimizar los gastos de marketing, identificando cu√°les son las fuentes de adquisici√≥n y estrategias que permiten maximizar el retorno de la inversi√≥n (ROI) y el valor del cliente a lo largo del tiempo (LTV). Para ello, se utilizan distintos conjuntos de datos:

Datos de visitas: Informaci√≥n sobre las sesiones de usuarios en la p√°gina web, con detalles como dispositivo utilizado, tiempo de inicio y fin de sesi√≥n, y fuente de adquisici√≥n.

Datos de √≥rdenes: Registro de las compras realizadas, que incluye la fecha de compra, el revenue generado y el identificador √∫nico del usuario.

Datos de costos: Informaci√≥n sobre el gasto en marketing, segmentada por fuente de adquisici√≥n y distribuida a lo largo del tiempo.

El an√°lisis se realiza a trav√©s de un notebook que gu√≠a al usuario en el proceso de transformaci√≥n, exploraci√≥n y an√°lisis de estos datos, permitiendo obtener insights clave para mejorar la eficiencia de las inversiones en marketing.

## Proceso de preprocesamiento
El preprocesamiento de los datos fue fundamental para asegurar la calidad del an√°lisis y se llev√≥ a cabo siguiendo estos pasos:

Carga y revisi√≥n inicial:
- Se importaron los datasets (visitas, √≥rdenes y costos) en formato CSV.
- Se visualiz√≥ una muestra de los datos y se revis√≥ la informaci√≥n general para identificar posibles valores faltantes o inconsistencias.
- Limpieza y transformaci√≥n:
- Se estandarizaron los nombres de las columnas a formato snake_case para facilitar el manejo de la informaci√≥n.
- Se convirtieron las columnas de fechas a formato datetime para poder realizar operaciones temporales.
- Se verificaron y eliminaron posibles duplicidades o registros err√≥neos, asegurando la integridad de la informaci√≥n.
- Generaci√≥n de variables adicionales:
- Se crearon nuevas columnas para facilitar el an√°lisis, como la duraci√≥n de la sesi√≥n (calculada como la diferencia entre el inicio y fin de la sesi√≥n) y agrupaciones temporales (por d√≠a, semana y mes).
- Se realiz√≥ una segmentaci√≥n mediante cohortes para analizar la retenci√≥n de usuarios y el comportamiento a lo largo del tiempo.
- Se combin√≥ la informaci√≥n de visitas, √≥rdenes y costos para obtener una visi√≥n integral de la relaci√≥n entre gasto en marketing, adquisici√≥n de clientes y su posterior comportamiento (como el revenue generado).

Visualizaci√≥n y an√°lisis exploratorio:
- Se generaron gr√°ficos para visualizar la distribuci√≥n de visitas (DAU, WAU, MAU), la duraci√≥n de las sesiones y la retenci√≥n de usuarios a lo largo del tiempo.
- Se analizaron los patrones de compra, identificando la temporalidad y frecuencia con la que los usuarios realizaban √≥rdenes, as√≠ como el revenue por cohorte.

## Hip√≥tesis general
Con base en el an√°lisis realizado se plantea la siguiente hip√≥tesis general:

Hip√≥tesis:

Optimizar los gastos de marketing focaliz√°ndose en las fuentes de adquisici√≥n que generan mayor conversi√≥n y presentan un menor costo de adquisici√≥n de clientes (CAC) permitir√° aumentar significativamente el retorno de inversi√≥n (ROI) y el valor a largo plazo del cliente (LTV).
Fundamentaci√≥n:
- Comportamiento de usuarios: Los an√°lisis muestran que la mayor√≠a de las visitas provienen de dispositivos desktop, lo que sugiere una mayor intenci√≥n de compra en este segmento. 
- Retenci√≥n y conversi√≥n: Se evidencia que los clientes que realizan la primera compra en un periodo corto tras su primera visita tienden a generar un mayor revenue acumulado, lo que se traduce en mejores indicadores de retenci√≥n.
- Optimizaci√≥n de inversi√≥n: Al identificar las fuentes de adquisici√≥n que, a pesar de generar un mayor gasto en marketing, convierten a un mayor n√∫mero de clientes con menor CAC, se puede reorientar la inversi√≥n para maximizar el retorno y la eficiencia de los recursos asignados a campa√±as publicitarias.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S9-analisis-de-negocio/analisis-showz_3-0.ipynb)

---
# Proyecto 8 SQL  

## Descripci√≥n del proyecto
El proyecto consiste en analizar datos de viajes en taxi en Chicago para entender patrones de comportamiento y probar hip√≥tesis relacionadas con la duraci√≥n de los viajes en funci√≥n de las condiciones clim√°ticas. El an√°lisis se basa en tres archivos CSV obtenidos mediante consultas SQL:

project_sql_result_01.csv ‚Äì Datos sobre el n√∫mero de viajes por compa√±√≠a de taxis el 15 y 16 de noviembre de 2017.
project_sql_result_04.csv ‚Äì Datos sobre el promedio de viajes que terminaron en cada barrio de Chicago en noviembre de 2017.
project_sql_result_07.csv ‚Äì Datos sobre viajes desde el Loop hasta el Aeropuerto Internacional O'Hare, incluyendo fecha y hora, condiciones clim√°ticas y duraci√≥n del viaje.

El proyecto incluye los siguientes objetivos:
-  Importar y limpiar los datos.
-  Realizar an√°lisis exploratorio para identificar patrones y tendencias.
-  Identificar las 10 principales compa√±√≠as de taxis y barrios por n√∫mero de viajes.
-  Visualizar los resultados mediante gr√°ficos.
-  Probar la hip√≥tesis sobre la relaci√≥n entre la duraci√≥n de los viajes y las condiciones clim√°ticas en s√°bados lluviosos.

## Proceso de preprocesamiento
El preprocesamiento de datos incluy√≥ las siguientes tareas:
1. Carga de datos: Se usaron las librer√≠as pandas, numpy, seaborn, matplotlib y scipy para importar y manipular los datos.
2. Revisi√≥n de datos: Se verific√≥ que los datos no tuvieran valores nulos o duplicados.
3. Correcci√≥n de tipos de datos: Se convirti√≥ el campo de fecha y hora (start_ts) al formato datetime.
4. Eliminaci√≥n de duplicados: Se eliminaron 197 registros duplicados en el archivo de viajes.
5. Tratamiento de valores at√≠picos: Se us√≥ el rango intercuartil (IQR) para eliminar valores extremos en la duraci√≥n de los viajes.
6. Agrupaci√≥n y ordenaci√≥n: Se agruparon los datos por compa√±√≠a de taxis y barrios para identificar los 10 principales en cada caso.

## Hip√≥tesis general
Hip√≥tesis nula (H‚ÇÄ): La duraci√≥n promedio de los viajes desde el Loop hasta el Aeropuerto Internacional O'Hare NO cambia significativamente los s√°bados lluviosos en comparaci√≥n con los s√°bados no lluviosos.

Hip√≥tesis alternativa (H‚ÇÅ): La duraci√≥n promedio de los viajes desde el Loop hasta el Aeropuerto Internacional O'Hare cambia significativamente los s√°bados lluviosos en comparaci√≥n con los s√°bados no lluviosos.

Para comprobar la hip√≥tesis:

-  Se realiz√≥ una prueba de Levene para verificar si las varianzas de las duraciones son iguales.
-  Se aplic√≥ una prueba t-test para determinar si hay diferencias significativas entre las medias.
-  Se estableci√≥ un nivel de significancia de 0.05 para la prueba.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S8-sql/analisis%20taxis%20dias%20lluviosos.ipynb)

---
# Proyecto 7 Aplicacion Web

## Descripci√≥n de los datos del proyecto
El proyecto se centra en crear una aplicaci√≥n web utilizando Streamlit para visualizar datos de anuncios de coches. El conjunto de datos (vehicles_us.csv) contiene informaci√≥n detallada sobre veh√≠culos en venta en Estados Unidos. El objetivo es desarrollar habilidades pr√°cticas en:

Configuraci√≥n de entornos virtuales en Python.
An√°lisis exploratorio de datos (EDA) mediante pandas y plotly-express.
Creaci√≥n de una aplicaci√≥n web interactiva con Streamlit para presentar los resultados.
Despliegue de la aplicaci√≥n en un servicio en la nube (Render).

Los datos incluyen informaci√≥n sobre:

- Precio del veh√≠culo.
- A√±o de fabricaci√≥n.
- Marca y modelo.
- Estado (nuevo o usado).
- Tipo de combustible.
- Kilometraje.
- Ubicaci√≥n geogr√°fica, entre otros.
- Pasos del Preprocesamiento de Datos
- Carga de datos
- Leer el archivo CSV (vehicles_us.csv) usando pandas y cargarlo en un DataFrame.
- Inspeccionar el conjunto de datos con info() y head() para verificar la estructura y tipos de datos.
- Limpieza de datos
- Eliminar valores duplicados y filas con valores nulos si es necesario.
- Convertir columnas de fechas a formato datetime.
- Corregir errores tipogr√°ficos o de formato en las columnas de texto (por ejemplo, marcas o modelos).
- Transformaci√≥n de datos
- Crear columnas adicionales si es necesario (por ejemplo, calcular el precio promedio por a√±o o por tipo de veh√≠culo).
- Redondear valores num√©ricos si es necesario.
- Agregaci√≥n de datos
- Agrupar los datos por categor√≠a (por ejemplo, por marca o tipo de combustible) para facilitar el an√°lisis.
- Calcular estad√≠sticas descriptivas (promedio, mediana, desviaci√≥n est√°ndar).
- Validaci√≥n de datos
- Comprobar que las transformaciones y la limpieza se realizaron correctamente.
- Asegurarse de que no haya valores nulos o datos inconsistentes.


## Hip√≥tesis

1. Hip√≥tesis sobre el precio de los veh√≠culos
2.  Hip√≥tesis nula (H‚ÇÄ): El precio promedio de los veh√≠culos no var√≠a significativamente entre diferentes marcas.
3. Hip√≥tesis alternativa (H‚ÇÅ): El precio promedio de los veh√≠culos var√≠a significativamente entre diferentes marcas.
4. Hip√≥tesis sobre el a√±o de fabricaci√≥n y el precio
5. Hip√≥tesis nula (H‚ÇÄ): No existe una correlaci√≥n significativa entre el a√±o de fabricaci√≥n y el precio del veh√≠culo.
6. Hip√≥tesis alternativa (H‚ÇÅ): Existe una correlaci√≥n significativa entre el a√±o de fabricaci√≥n y el precio del veh√≠culo.

[Ver Proyecto](https://notebooks-o54z.onrender.com/)

---

# Proyecto 6 An√°lisis de Ventas de Juegos de Consola üéÆüìä

## Descripci√≥n de datos

Descripci√≥n General del Proyecto
El proyecto consiste en un an√°lisis de datos de ventas de videojuegos para la tienda online Ice, que vende videojuegos a nivel mundial. El objetivo principal es identificar patrones y factores que determinan el √©xito de un videojuego, con el fin de planificar campa√±as publicitarias efectivas y detectar proyectos prometedores para el futuro. El an√°lisis se basa en un conjunto de datos que abarca desde 2016, y se asume que el an√°lisis se realiza en diciembre de 2016 para planificar estrategias para el a√±o 2017.


## Preprocesamiento de datos
1. Revisi√≥n de datos faltantes o inconsistentes:
A√±o de lanzamiento: Hay valores faltantes en Year_of_Release que deben manejarse.
Ventas globales y regionales: Valores nulos o vac√≠os podr√≠an indicar datos no registrados.

2. Conversi√≥n de tipos de datos:
Year_of_Release puede necesitar conversi√≥n a un formato num√©rico o de fecha.
Las columnas de ventas (NA_Sales, etc.) podr√≠an necesitar conversi√≥n a tipo num√©rico para realizar c√°lculos correctamente.

3. Creaci√≥n de nuevas variables:
A√±os desde el lanzamiento: Crear una columna calculada para saber cu√°ntos a√±os han pasado desde que el juego fue lanzado.
Ventas totales: Verificar que Global_Sales sea la suma de las ventas regionales; si no, calcularla.

4. Eliminaci√≥n de outliers:
Revisar las ventas extremadamente altas o bajas para evitar distorsiones en el an√°lisis.

5. Normalizaci√≥n o estandarizaci√≥n:

Para an√°lisis comparativos, podr√≠a ser necesario escalar las ventas por regi√≥n.

## Hip√≥tesis
Al trabajar con datos relacionados con videojuegos, podr√≠amos plantear las siguientes hip√≥tesis:

1.- Hip√≥tesis de ventas por regi√≥n:
"Los videojuegos tienen mayores ventas en Norteam√©rica que en otras regiones, debido a la alta penetraci√≥n de consolas en este mercado."

2.- Hip√≥tesis sobre g√©neros:
"Los g√©neros de acci√≥n y deportes generan mayores ventas globales que otros g√©neros debido a su popularidad masiva."

3.- Hip√≥tesis sobre plataformas:
"Los juegos lanzados en plataformas de √∫ltima generaci√≥n (e.g., PS4, Xbox One) tienen mayores ventas globales que los lanzados en plataformas antiguas."

4.-Hip√≥tesis sobre lanzamientos recientes:
"Los videojuegos lanzados despu√©s de 2015 tienen un mayor promedio de ventas globales debido a la creciente demanda de videojuegos."

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S6-gamestore/game-6.ipynb)

---
# Proyecto 5 Megaline Telecom

Descripci√≥n General del Proyecto
El proyecto consiste en analizar los ingresos generados por dos planes tarifarios (Surf y Ultimate) ofrecidos por Megaline, una empresa de telecomunicaciones. El objetivo es determinar cu√°l de los dos planes genera m√°s ingresos para poder ajustar el presupuesto de publicidad de la empresa. Para ello, se dispone de datos sobre 500 clientes, que incluyen informaci√≥n sobre las llamadas, mensajes y datos consumidos en 2018, as√≠ como el plan al que est√°n suscritos.

## Preprocesamiento de los datos

1. Carga de datos
2. Cargar las cinco tablas de datos (users, calls, messages, internet, plans) en DataFrames de pandas.
3. Revisi√≥n y limpieza de datos
4. Inspeccionar los datos para identificar valores nulos y duplicados.
5. Corregir los tipos de datos (por ejemplo, convertir fechas de strings a datetime).
6. Completar o eliminar valores nulos seg√∫n corresponda.
7. Redondeo de datos
8. Redondear la duraci√≥n de las llamadas hacia arriba al minuto m√°s cercano.
9. Redondear los datos de tr√°fico de internet al GB m√°s cercano.
10. Creaci√≥n de columnas adicionales
11. Extraer el mes de las fechas para facilitar el an√°lisis mensual.
12. Agregaci√≥n de datos por usuario y mes
13. N√∫mero total de llamadas, mensajes y datos utilizados por cada usuario al mes.
14. Crear una tabla maestra que combine todos los datos en un solo DataFrame.
15. C√°lculo de ingresos mensuales
16. Restar el l√≠mite incluido en el plan al consumo real.
17. Cobrar los minutos, mensajes y datos extra.
18. Sumar la tarifa mensual del plan para obtener el ingreso mensual total.


## Descripci√≥n de la hip√≥tesis

- Hip√≥tesis sobre los ingresos por tarifa
- Hip√≥tesis nula (H‚ÇÄ): El ingreso promedio de los usuarios de los planes Surf y Ultimate es igual.
- Hip√≥tesis alternativa (H‚ÇÅ): El ingreso promedio de los usuarios de los planes Surf y Ultimate es diferente.
- Hip√≥tesis sobre los ingresos por regi√≥n
- Hip√≥tesis nula (H‚ÇÄ): El ingreso promedio de los usuarios en la regi√≥n de Nueva York-Nueva Jersey es igual al ingreso  promedio en otras regiones.
- Hip√≥tesis alternativa (H‚ÇÅ): El ingreso promedio de los usuarios en la regi√≥n de Nueva York-Nueva Jersey es diferente al   ingreso promedio en otras regiones.
- Criterio de prueba

Se utilizar√° una prueba t para muestras independientes para evaluar la diferencia entre los ingresos promedios.
El valor de significancia (Œ±) se establecer√° en 0.05.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S5-megaline-cellphone/compa%C3%B1ia-telefonica.ipynb)


---
# Proyecto 4 Data Wrangling Instacart Grosery üõí
## Descripci√≥n de los datos
El dataset contiene informaci√≥n transaccional de un supermercado en l√≠nea (Instacart). Los datos incluyen:

√ìrdenes: Informaci√≥n sobre las compras realizadas por los clientes (ID de cliente, n√∫mero de pedido, etc.).
Productos: Listado de productos disponibles (nombres, categor√≠as, etc.).
Departamentos: Clasificaci√≥n de los productos seg√∫n categor√≠as o departamentos.
Datos temporales: Informaci√≥n sobre horarios y d√≠as en los que se realizan las compras.
Los datos son ricos en variables relacionadas con los h√°bitos de compra de los clientes, lo que facilita el an√°lisis de comportamiento del consumidor.

## Preprocesamiento de los datos
1.- Carga de datos: Lectura de archivos CSV (u otro formato) en estructuras de datos como DataFrames de Pandas.

2.-Limpieza de datos:
Eliminaci√≥n de valores nulos o duplicados.
Conversi√≥n de tipos de datos seg√∫n necesidad.

3.-Transformaciones:
Creaci√≥n de variables nuevas, como el tiempo entre √≥rdenes consecutivas o la frecuencia de compra de ciertos productos.
Uni√≥n de tablas utilizando claves comunes (por ejemplo, product_id o order_id).

4.-Normalizaci√≥n: Cambio en los nombres de columnas para uniformidad y mejor comprensi√≥n.

## Hip√≥tesis
1.-Patrones de recompra: Los clientes tienden a comprar productos de ciertas categor√≠as con mayor frecuencia durante los fines de semana.

2.-Preferencias por departamento: Los productos de ciertos departamentos (como frutas y verduras) tienen un mayor porcentaje de repetici√≥n en comparaci√≥n con otros.

3.-Relaci√≥n entre el tiempo y las compras: Los clientes con intervalos m√°s cortos entre pedidos probablemente sean clientes regulares con patrones de recompra estables.

4.-Ofertas y promociones: Los descuentos o promociones impactan significativamente en el volumen de compras.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S4-data-wrangling-instacart/instacart-grocery.ipynb)
---
# Proyecto 3 Data Wrangling Spotify  üé§üîä
## Descripci√≥n de los datos
El archivo contiene datos relacionados con la reproducci√≥n de m√∫sica en diferentes ciudades. Las columnas son las siguientes:

City: Nombre de la ciudad.
Day: D√≠a de la semana en que se registraron las reproducciones.
Genre: G√©nero musical reproducido.
Plays: N√∫mero de reproducciones.

## Preprocesamiento de datos
Valores ausentes: Se debe verificar si hay celdas vac√≠as en las columnas clave.
Duplicados: Se identifican filas duplicadas en los datos.
Revisi√≥n de formatos: Asegurarse de que las columnas tienen los tipos correctos (por ejemplo, n√∫meros en Plays).
Voy a realizar estos pasos a continuaci√≥n. ‚Äã‚Äã

** Resultados del preprocesamiento:

Valores ausentes: No hay valores ausentes en ninguna de las columnas.
Duplicados: No hay filas duplicadas en los datos.
Tipos de datos:

City, Day, Genre: Tipo object (texto), correcto para estas variables.
Plays: Tipo int64, adecuado para el n√∫mero de reproducciones.
Los datos est√°n limpios y no requieren modificaciones en estas √°reas.

## Prueba de hip√≥tesis
Para el an√°lisis, plantearemos la siguiente hip√≥tesis:

Hip√≥tesis: La cantidad promedio de reproducciones de m√∫sica en "City A" es diferente de la cantidad promedio en "City B".
Voy a calcular las medias y realizar una prueba estad√≠stica (t-test) para comparar. ‚Äã‚Äã

Parece que no puedo realizar an√°lisis de datos m√°s avanzados en este momento. Por favor, int√©ntalo de nuevo m√°s tarde o proporciona m√°s detalles si necesitas asistencia adicional con los resultados o an√°lisis.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S3-data-wrangling-spotify/data-wrangling-spotify.ipynb)

---

# Proyecto 2 Analisis de Store Basico

## Descripci√≥n de datos
Es el an√°lisis exploratorio y estad√≠stico inicial que se realiza sobre un conjunto de datos. Este proceso incluye:

Verificar las dimensiones del dataset (n√∫mero de filas y columnas).
Identificar los tipos de datos de cada variable (num√©rica, categ√≥rica, etc.).
Examinar las estad√≠sticas descriptivas (media, mediana, moda, desviaci√≥n est√°ndar, valores m√≠nimos y m√°ximos).
Detectar datos faltantes o valores at√≠picos (outliers).
Visualizar distribuciones y relaciones entre variables.

## Preprocesamiento de datos
Es el conjunto de t√©cnicas aplicadas para limpiar, transformar y preparar los datos para un an√°lisis o modelo. Incluye:

Manejo de datos faltantes (imputaci√≥n o eliminaci√≥n).
Normalizaci√≥n o escalamiento de variables.
Codificaci√≥n de variables categ√≥ricas (como one-hot encoding).
Eliminaci√≥n de duplicados.
Transformaciones de datos (logaritmos, escalamiento, etc.).
Reducci√≥n de dimensionalidad o selecci√≥n de variables relevantes.

## Prueba de hip√≥tesis
Este proyecto se hizo limpieza general de toda la informacion asi como se creo funciones para simplicar nuestro trabajo de limpieza.

[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S2-analisis-venta-basico-2/analis-venta-basico-2.ipynb)

---

# Proyecto 1 Analisis de Store Basico

## Descripci√≥n de los datos
El conjunto de datos incluye una lista de usuarios con los siguientes atributos:

ID del usuario (user_id): Identificador √∫nico (cadena).
Nombre del usuario (user_name): Nombre del cliente (cadena, incluye espacios y posibles inconsistencias como guiones bajos).
Edad (user_age): Edad del usuario (n√∫mero flotante).
Categor√≠as favoritas (fav_categories): Lista de categor√≠as de productos que el usuario prefiere.
Gastos por categor√≠a: Lista de montos gastados en cada categor√≠a favorita (n√∫meros enteros).

Ejemplo de registro:

['32415', ' mike_reed ', 32.0, ['ELECTRONICS', 'SPORT', 'BOOKS'], [894, 213, 173]]

## Preprocesamiento de datos
Se identificaron acciones necesarias:

1. Limpieza de nombres:
Eliminar espacios en blanco al inicio y al final.
Sustituir caracteres como guiones bajos (_) por espacios para mejorar la legibilidad.
Capitalizar los nombres para mantener un formato consistente (por ejemplo, "JOHN DOE" ‚Üí "John Doe").

Validaci√≥n de categor√≠as y gastos:
Verificar que cada usuario tenga el mismo n√∫mero de categor√≠as favoritas y montos de gasto asociados. Si no coinciden, se eliminar√≠an esos registros.
Ejemplo transformado:

Antes: ['32415', ' mike_reed ', 32.0, ['ELECTRONICS', 'SPORT', 'BOOKS'], [894, 213, 173]]
Despu√©s: ['32415', 'Mike Reed', 32.0, ['ELECTRONICS', 'SPORT', 'BOOKS'], [894, 213, 173]]

2. Exploraci√≥n de datos
Una vez limpios, habr√≠a revisado los datos para comprender mejor las tendencias:

Distribuci√≥n de edades: Analizar la media, mediana y rango de las edades de los usuarios.
Categor√≠as favoritas m√°s comunes: Identificar cu√°les categor√≠as son m√°s populares.
Relaci√≥n entre edad y categor√≠as favoritas: Explorar si hay correlaci√≥n entre la edad y las preferencias.

## Hip√≥tesis
En este proyecto fue iniciandome en el mundo de analisis de datos siendo la parte de limpieza asi como ordenar, trasformar a minusculas o mayusculas, cambiar a enteros, etc... fue solo limpieza y procesamiento de datos buen inicio para el mundo de analista de datos.



[Ver Proyecto](https://github.com/lozaner/DataAnalytics/blob/main/portfolio-DA/S1-analisis-tienda-basico-1/store_1.ipynb)
---

## üë®‚Äçüíª Autor
Creado por [Elpidio Lozano](https://github.com/lozaner).  
Para dudas o sugerencias, ¬°cont√°ctame! üòä

---
